{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dallas Agent Workshop - Colab Quickstart\n",
    "\n",
    "This notebook runs the workshop repo in Google Colab.\n",
    "\n",
    "Steps:\n",
    "1. Clone the repo + install dependencies\n",
    "2. Set `OPENROUTER_API_KEY` (and optionally `TAVILY_API_KEY`)\n",
    "3. Run the preflight + demo calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- 5:30 - Check-in, food, networking (30)\n",
    "- 6:00 - Why Agents? (Motivation & Framing) (5)\n",
    "- 6:05 - Core Concepts & Architectures (5)\n",
    "- 6:10 - Setup & Environment (30)\n",
    "- 6:40 - Live Code Walkthrough (20)\n",
    "- 7:00 - Hands-On Build Session (40)\n",
    "- 7:40 - Engineering Discipline for Agents (10)\n",
    "- 7:50 - Next: Virtual sessions, submitting PRs (10)\n",
    "- 8:00 - Curated Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Agents? (Motivation & Framing)\n",
    "\n",
    "An agent is a loop that can **plan**, **use tools**, and **iterate** toward a goal. In this workshop, the agent can:\n",
    "- Write Python code\n",
    "- Execute it in a controlled way\n",
    "- Read the result (stdout/stderr) and try again\n",
    "\n",
    "Why this matters: lots of real work is not a single prompt. It needs multi-step problem solving, verification, and guardrails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts & Architectures\n",
    "\n",
    "We will use a simple LangGraph workflow that looks like:\n",
    "\n",
    "`plan -> exec -> (fix -> exec)* -> finish`\n",
    "\n",
    "Key ideas:\n",
    "- **State**: the data that flows between steps (task, generated code, last run result).\n",
    "- **Tools**: the controlled actions the agent can take (here: running Python; for research: web search).\n",
    "- **Guardrails**: constraints that keep the agent safe/reliable (timeouts, blocked imports, \"must print\" requirement).\n",
    "- **Evaluation mindset**: make the agent produce observable outputs so you can debug quickly (stdout, logs, reproducible steps).\n",
    "\n",
    "During the live walkthrough, we will inspect `agent_lib.py` and connect each node to what you see on screen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jiankunliu-ai/dallas-ai-agent-workshop.git\n",
    "%cd dallas-ai-agent-workshop\n",
    "!pip -q install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API Keys\n",
    "\n",
    "Use Colab's prompt to avoid hardcoding secrets into the notebook.\n",
    "\n",
    "If you get a 401 later, restart the runtime and rerun this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "openrouter_key = getpass('OPENROUTER_API_KEY: ').strip()\n",
    "if not openrouter_key:\n",
    "    raise RuntimeError('OPENROUTER_API_KEY is required')\n",
    "\n",
    "os.environ['OPENROUTER_API_KEY'] = openrouter_key\n",
    "os.environ['OPENROUTER_MODEL'] = 'arcee-ai/trinity-large-preview:free'\n",
    "\n",
    "tavily_key = getpass('TAVILY_API_KEY (optional, press enter to skip): ').strip()\n",
    "if tavily_key:\n",
    "    os.environ['TAVILY_API_KEY'] = tavily_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_lib import run_task\n",
    "\n",
    "task = \"Write a Python function to compute Fibonacci(n) efficiently and print Fibonacci(35).\"\n",
    "result = run_task(task)\n",
    "\n",
    "result['last_run']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_lib import run_task\n",
    "\n",
    "tasks = [\n",
    "    \"Parse this CSV string and compute the average of the 'latency_ms' column:\\n\\nts,latency_ms\\n1,120\\n2,110\\n3,130\\n4,90\\n\",\n",
    "    \"Implement rolling z-score anomaly score for this list and print the top 3 most anomalous points: [10,11,9,10,10,200,11,10,9,10]\",\n",
    "    \"Given a list of (user_id, event_time, event_type), compute per-user session counts (30-min gap) and print a dict.\"\n",
    "]\n",
    "\n",
    "for t in tasks:\n",
    "    print('\\n' + '='*80)\n",
    "    print('TASK:', t)\n",
    "    out = run_task(t)\n",
    "    print('OK:', out['last_run']['ok'])\n",
    "    print('STDOUT:\\n', out['last_run']['stdout'])\n",
    "    if not out['last_run']['ok']:\n",
    "        print('STDERR:\\n', out['last_run']['stderr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Engineering Discipline for Agents\n",
    "\n",
    "Agents feel \"magical\" until they fail. The fastest way to make them reliable is good engineering hygiene:\n",
    "\n",
    "- **State management**: write down what the agent knows (inputs/outputs) and pass it explicitly.\n",
    "- **Context management**: keep prompts short and structured; include only what is necessary; summarize when needed.\n",
    "- **Memory (optional)**: decide what should persist across runs (none vs. per-session vs. long-term).\n",
    "- **Token budgeting**: constrain output formats; avoid dumping large logs or huge documents into the model.\n",
    "- **Governance & safety**: limit tools and permissions; log tool calls; treat credentials carefully; assume untrusted outputs.\n",
    "\n",
    "In this repo, we keep things workshop-safe by forcing observable stdout, logging generated code, adding timeouts, and blocking risky Python calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Next: Virtual sessions + Submitting PRs\n",
    "\n",
    "Stretch goals (great for follow-up sessions):\n",
    "- Add more tools (file I/O, data APIs) with careful safety boundaries\n",
    "- Turn the workflow into multiple collaborating agents (planner + specialists)\n",
    "- Add lightweight evaluations (golden tests, regression prompts, success criteria)\n",
    "\n",
    "If you improve the workshop materials, please open a PR against this repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Curated Resources\n",
    "\n",
    "- Prompting best practices (Claude): https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-prompting-best-practices\n",
    "- LangGraph docs: https://langchain-ai.github.io/langgraph/\n",
    "- OpenRouter docs: https://openrouter.ai/docs\n",
    "\n",
    "Tip: when learning, keep a small set of repeatable test prompts (like `2+2`, Fibonacci, CSV parse) to validate changes quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_agent import run_research\n",
    "\n",
    "question = \"What are the top 3 AI chip companies in 2024 and what's their competitive advantage?\"\n",
    "\n",
    "print(f\"RESEARCH QUESTION:\\n{question}\\n\")\n",
    "\n",
    "result = run_research(question)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('FINAL REPORT:')\n",
    "print('='*60)\n",
    "print(result[\"report\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
